{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "np.random.seed(42)\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yusuf/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAACDCAYAAACp4J7uAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAHFBMAQAAOPgXNIeXh9H25HoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1416289860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_samples = 5\n",
    "\n",
    "plt.figure(figsize=(n_samples * 2, 3))\n",
    "for index in range(n_samples):\n",
    "    plt.subplot(1, n_samples, index + 1)\n",
    "    sample_image = mnist.train.images[index].reshape(28, 28)\n",
    "    plt.imshow(sample_image, cmap=\"binary\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 3, 4, 6, 1], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.labels[:n_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input images\n",
    "X = tf.placeholder(shape=[None,28,28,1],dtype=tf.float32,name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Primary Capsules\n",
    "caps1_n_maps = 32\n",
    "caps1_n_caps = caps1_n_maps * 6 * 6  # 1152 primary capsules\n",
    "caps1_n_dims = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters for fist caps layer\n",
    "conv1_params = {\n",
    "    \"filters\": 256,\n",
    "    \"kernel_size\": 9,\n",
    "    \"strides\": 1,\n",
    "    \"padding\": \"valid\",\n",
    "    \"activation\": tf.nn.relu,\n",
    "}\n",
    "\n",
    "# Parameters for second caps layer\n",
    "conv2_params = {\n",
    "    \"filters\": caps1_n_maps * caps1_n_dims, # 256 convolutional filters\n",
    "    \"kernel_size\": 9,\n",
    "    \"strides\": 2,\n",
    "    \"padding\": \"valid\",\n",
    "    \"activation\": tf.nn.relu\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fitting two convolution layers\n",
    "conv1 = tf.layers.conv2d(X, name=\"conv1\", **conv1_params) # output 256 feature map for each instance has 20x20\n",
    "conv2 = tf.layers.conv2d(conv1, name=\"conv2\", **conv2_params) # output 256 feature map for each instance has 6X6. stride 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generating capsules 32 of 8 dimention. total = 32X6X6\n",
    "caps1_raw = tf.reshape(conv2,[-1, caps1_n_caps, caps1_n_dims],name=\"caps1_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sampling: squash \n",
    "def squash(s, axis=-1, epsilon=1e-7, name=None):\n",
    "    with tf.name_scope(name, default_name=\"squash\"):\n",
    "        squared_norm = tf.reduce_sum(tf.square(s), axis=axis,\n",
    "                                     keep_dims=True)\n",
    "        safe_norm = tf.sqrt(squared_norm + epsilon)\n",
    "        squash_factor = squared_norm / (1. + squared_norm)\n",
    "        unit_vector = s / safe_norm\n",
    "        return squash_factor * unit_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "caps1_output = squash(caps1_raw, name=\"caps1_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caps2_n_caps = 10\n",
    "caps2_n_dims = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Digit Capsule. \n",
    "# preparing tranformation matrix 16X8\n",
    "\n",
    "init_sigma = 0.1\n",
    "\n",
    "W_init = tf.random_normal(\n",
    "    shape=(1, caps1_n_caps, caps2_n_caps, caps2_n_dims, caps1_n_dims),\n",
    "    stddev=init_sigma, dtype=tf.float32, name=\"W_init\")\n",
    "W = tf.Variable(W_init, name=\"W\")\n",
    "\n",
    "batch_size = tf.shape(X)[0]\n",
    "W_tiled = tf.tile(W, [batch_size, 1, 1, 1, 1], name=\"W_tiled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preparing primary capsule\n",
    "caps1_output_expanded = tf.expand_dims(caps1_output, -1,\n",
    "                                       name=\"caps1_output_expanded\")\n",
    "caps1_output_tile = tf.expand_dims(caps1_output_expanded, 2,\n",
    "                                   name=\"caps1_output_tile\")\n",
    "caps1_output_tiled = tf.tile(caps1_output_tile, [1, 1, caps2_n_caps, 1, 1],\n",
    "                             name=\"caps1_output_tiled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# multiplication of two arrays\n",
    "caps2_predicted = tf.matmul(W_tiled, caps1_output_tiled,\n",
    "                            name=\"caps2_predicted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'caps2_output_round_1/mul:0' shape=(?, 1, 10, 16, 1) dtype=float32>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Routing by agreement. in this layer we make prediction vector for each class\n",
    "# initialize the raw routing weights $b_{i,j}$ to zero:\n",
    "raw_weights = tf.zeros([batch_size, caps1_n_caps, caps2_n_caps, 1, 1],dtype=np.float32, name=\"raw_weights\")\n",
    "\n",
    "# apply the softmax function to compute the routing weights\n",
    "routing_weights = tf.nn.softmax(raw_weights, dim=2, name=\"routing_weights\")\n",
    "\n",
    "# compute the weighted sum of all the predicted output vectors for each second-layer capsule\n",
    "weighted_predictions = tf.multiply(routing_weights, caps2_predicted,name=\"weighted_predictions\")\n",
    "weighted_sum = tf.reduce_sum(weighted_predictions, axis=1, keep_dims=True,name=\"weighted_sum\")\n",
    "\n",
    "# finally, let's apply the squash function to get the outputs of the second layer capsules\n",
    "caps2_output_round_1 = squash(weighted_sum, axis=-2,name=\"caps2_output_round_1\")\n",
    "\n",
    "# output after first round \n",
    "caps2_output_round_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Round 2. \n",
    "caps2_predicted\n",
    "caps2_output_round_1\n",
    "\n",
    "# let's measure how close each predicted vector to the actual output vector \n",
    "caps2_output_round_1_tiled = tf.tile(caps2_output_round_1, [1, caps1_n_caps, 1, 1, 1],\n",
    "                                     name=\"caps2_output_round_1_tiled\")\n",
    "agreement = tf.matmul(caps2_predicted, caps2_output_round_1_tiled,transpose_a=True, name=\"agreement\")\n",
    "\n",
    "# update the raw routing weights by simply adding the scalar product of predicted and actual output\n",
    "raw_weights_round_2 = tf.add(raw_weights, agreement,name=\"raw_weights_round_2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "routing_weights_round_2 = tf.nn.softmax(raw_weights_round_2,dim=2,name=\"routing_weights_round_2\")\n",
    "\n",
    "weighted_predictions_round_2 = tf.multiply(routing_weights_round_2,caps2_predicted,name=\"weighted_predictions_round_2\")\n",
    "\n",
    "weighted_sum_round_2 = tf.reduce_sum(weighted_predictions_round_2,axis=1, keep_dims=True,name=\"weighted_sum_round_2\")\n",
    "\n",
    "caps2_output_round_2 = squash(weighted_sum_round_2,axis=-2,name=\"caps2_output_round_2\")\n",
    "\n",
    "\n",
    "caps2_output = caps2_output_round_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'y_proba_1:0' shape=(?, 1, 1) dtype=int64>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estimated Class Probabilities (Length)\n",
    "def safe_norm(s, axis=-1, epsilon=1e-7, keep_dims=False, name=None):\n",
    "    with tf.name_scope(name, default_name=\"safe_norm\"):\n",
    "        squared_norm = tf.reduce_sum(tf.square(s), axis=axis,\n",
    "                                     keep_dims=keep_dims)\n",
    "        return tf.sqrt(squared_norm + epsilon)\n",
    "\n",
    "y_proba = safe_norm(caps2_output, axis=-2, name=\"y_proba\")\n",
    "\n",
    "# To predict the class of each instance, we can just select the one with the highest estimated probability\n",
    "y_proba_argmax = tf.argmax(y_proba, axis=2, name=\"y_proba\")\n",
    "y_proba_argmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'y_pred:0' shape=(?,) dtype=int64>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = tf.squeeze(y_proba_argmax, axis=[1,2], name=\"y_pred\")\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Okay, we are now ready to define the training operations, starting with the losses.\n",
    "y = tf.placeholder(shape=[None], dtype=tf.int64, name=\"y\")\n",
    "\n",
    "# Margin loss\n",
    "m_plus = 0.9\n",
    "m_minus = 0.1\n",
    "lambda_ = 0.5\n",
    "\n",
    "T = tf.one_hot(y, depth=caps2_n_caps, name=\"T\")\n",
    "with tf.Session():\n",
    "    print(T.eval(feed_dict={y: np.array([0, 1, 2, 3, 9])}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'caps2_output_round_2/mul:0' shape=(?, 1, 10, 16, 1) dtype=float32>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caps2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# margin loss L_k = T_k \\max(0, m^{+} - \\|\\mathbf{v}_k\\|)^2 + \\lambda (1 - T_k) \\max(0, \\|\\mathbf{v}_k\\| - m^{-})^2$\n",
    "\n",
    "caps2_output_norm = safe_norm(caps2_output, axis=-2, keep_dims=True,name=\"caps2_output_norm\")\n",
    "\n",
    "present_error_raw = tf.square(tf.maximum(0., m_plus - caps2_output_norm),name=\"present_error_raw\")\n",
    "\n",
    "present_error = tf.reshape(present_error_raw, shape=(-1, 10),name=\"present_error\")\n",
    "\n",
    "absent_error_raw = tf.square(tf.maximum(0., caps2_output_norm - m_minus),name=\"absent_error_raw\")\n",
    "\n",
    "absent_error = tf.reshape(absent_error_raw, shape=(-1, 10),name=\"absent_error\")\n",
    "\n",
    "\n",
    "# ready to compute the loss\n",
    "L = tf.add(T * present_error, lambda_ * (1.0 - T) * absent_error,name=\"L\")\n",
    "\n",
    "#sum the digit losses for each instance ($L_0 + L_1 + \\cdots + L_9$), and compute the mean over all instances. \n",
    "# This gives us the final margin loss\n",
    "margin_loss = tf.reduce_mean(tf.reduce_sum(L, axis=1), name=\"margin_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'reconstruction_mask:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reconstruction\n",
    "# Mask all output vector except the predicted output vector\n",
    "mask_with_labels = tf.placeholder_with_default(False, shape=(),name=\"mask_with_labels\")\n",
    "reconstruction_targets = tf.cond(mask_with_labels, # condition\n",
    "                                 lambda: y,        # if True\n",
    "                                 lambda: y_pred,   # if False\n",
    "                                 name=\"reconstruction_targets\")\n",
    "\n",
    "reconstruction_mask = tf.one_hot(reconstruction_targets,\n",
    "                                 depth=caps2_n_caps,\n",
    "                                 name=\"reconstruction_mask\")\n",
    "reconstruction_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'caps2_output_round_2/mul:0' shape=(?, 1, 10, 16, 1) dtype=float32>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caps2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'caps2_output_masked_1:0' shape=(?, 1, 10, 16, 1) dtype=float32>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruction_mask_reshaped = tf.reshape(reconstruction_mask, [-1, 1, caps2_n_caps, 1, 1],\n",
    "                                          name=\"reconstruction_mask_reshaped\")\n",
    "\n",
    "caps2_output_masked = tf.multiply(caps2_output, reconstruction_mask_reshaped,name=\"caps2_output_masked\")\n",
    "\n",
    "caps2_output_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_input = tf.reshape(caps2_output_masked,[-1, caps2_n_caps * caps2_n_dims],name=\"decoder_input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'decoder_input:0' shape=(?, 160) dtype=float32>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Decoder\n",
    "\n",
    "n_hidden1 = 512\n",
    "n_hidden2 = 1024\n",
    "n_output = 28 * 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "with tf.name_scope(\"decoder\"):\n",
    "    hidden1 = tf.layers.dense(decoder_input, n_hidden1,\n",
    "                              activation=tf.nn.relu,\n",
    "                              name=\"hidden1\")\n",
    "    \n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2,\n",
    "                              activation=tf.nn.relu,\n",
    "                              name=\"hidden2\")\n",
    "    \n",
    "    decoder_output = tf.layers.dense(hidden2, n_output,\n",
    "                                     activation=tf.nn.sigmoid,\n",
    "                                     name=\"decoder_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reconstruction Loss\n",
    "X_flat = tf.reshape(X, [-1, n_output], name=\"X_flat\")\n",
    "squared_difference = tf.square(X_flat - decoder_output,\n",
    "                               name=\"squared_difference\")\n",
    "reconstruction_loss = tf.reduce_mean(squared_difference,\n",
    "                                    name=\"reconstruction_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Final Loss\n",
    "alpha = 0.0005\n",
    "loss = tf.add(margin_loss, alpha * reconstruction_loss, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Final Touch\n",
    "# Accuracy \n",
    "correct = tf.equal(y, y_pred, name=\"correct\")\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "# Training Operations\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "training_op = optimizer.minimize(loss, name=\"training_op\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# we are done with construction phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  Val accuracy: 98.8400%  Loss: 0.015572 (improved)\n",
      "Epoch: 2  Val accuracy: 99.2200%  Loss: 0.010501 (improved)\n",
      "Epoch: 3  Val accuracy: 99.3200%  Loss: 0.008675 (improved)\n",
      "Epoch: 4  Val accuracy: 99.2200%  Loss: 0.009696\n",
      "Epoch: 5  Val accuracy: 99.1600%  Loss: 0.008130 (improved)\n",
      "Epoch: 6  Val accuracy: 99.2000%  Loss: 0.008332\n",
      "Epoch: 7  Val accuracy: 99.2600%  Loss: 0.007879 (improved)\n",
      "Epoch: 8  Val accuracy: 99.3400%  Loss: 0.006959 (improved)\n",
      "Epoch: 9  Val accuracy: 99.3000%  Loss: 0.007801\n",
      "Epoch: 10  Val accuracy: 99.1000%  Loss: 0.009339\n"
     ]
    }
   ],
   "source": [
    "# Training \n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 50\n",
    "restore_checkpoint = True\n",
    "\n",
    "n_iterations_per_epoch = mnist.train.num_examples // batch_size\n",
    "n_iterations_validation = mnist.validation.num_examples // batch_size\n",
    "best_loss_val = np.infty\n",
    "checkpoint_path = \"./my_capsule_network\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if restore_checkpoint and tf.train.checkpoint_exists(checkpoint_path):\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        init.run()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(1, n_iterations_per_epoch + 1):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            # Run the training operation and measure the loss:\n",
    "            _, loss_train = sess.run(\n",
    "                [training_op, loss],\n",
    "                feed_dict={X: X_batch.reshape([-1, 28, 28, 1]),\n",
    "                           y: y_batch,\n",
    "                           mask_with_labels: True})\n",
    "            print(\"\\rIteration: {}/{} ({:.1f}%)  Loss: {:.5f}\".format(\n",
    "                      iteration, n_iterations_per_epoch,\n",
    "                      iteration * 100 / n_iterations_per_epoch,\n",
    "                      loss_train),\n",
    "                  end=\"\")\n",
    "\n",
    "        # At the end of each epoch,\n",
    "        # measure the validation loss and accuracy:\n",
    "        loss_vals = []\n",
    "        acc_vals = []\n",
    "        for iteration in range(1, n_iterations_validation + 1):\n",
    "            X_batch, y_batch = mnist.validation.next_batch(batch_size)\n",
    "            loss_val, acc_val = sess.run(\n",
    "                    [loss, accuracy],\n",
    "                    feed_dict={X: X_batch.reshape([-1, 28, 28, 1]),\n",
    "                               y: y_batch})\n",
    "            loss_vals.append(loss_val)\n",
    "            acc_vals.append(acc_val)\n",
    "            print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(\n",
    "                      iteration, n_iterations_validation,\n",
    "                      iteration * 100 / n_iterations_validation),\n",
    "                  end=\" \" * 10)\n",
    "        loss_val = np.mean(loss_vals)\n",
    "        acc_val = np.mean(acc_vals)\n",
    "        print(\"\\rEpoch: {}  Val accuracy: {:.4f}%  Loss: {:.6f}{}\".format(\n",
    "            epoch + 1, acc_val * 100, loss_val,\n",
    "            \" (improved)\" if loss_val < best_loss_val else \"\"))\n",
    "\n",
    "        # And save the model if it improved:\n",
    "        if loss_val < best_loss_val:\n",
    "            save_path = saver.save(sess, checkpoint_path)\n",
    "            best_loss_val = loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_capsule_network\n",
      "Final test accuracy: 99.3700%  Loss: 0.006760   \n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "n_iterations_test = mnist.test.num_examples // batch_size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "\n",
    "    loss_tests = []\n",
    "    acc_tests = []\n",
    "    for iteration in range(1, n_iterations_test + 1):\n",
    "        X_batch, y_batch = mnist.test.next_batch(batch_size)\n",
    "        loss_test, acc_test = sess.run(\n",
    "                [loss, accuracy],\n",
    "                feed_dict={X: X_batch.reshape([-1, 28, 28, 1]),\n",
    "                           y: y_batch})\n",
    "        loss_tests.append(loss_test)\n",
    "        acc_tests.append(acc_test)\n",
    "        print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(\n",
    "                  iteration, n_iterations_test,\n",
    "                  iteration * 100 / n_iterations_test),\n",
    "              end=\" \" * 10)\n",
    "    loss_test = np.mean(loss_tests)\n",
    "    acc_test = np.mean(acc_tests)\n",
    "    print(\"\\rFinal test accuracy: {:.4f}%  Loss: {:.6f}\".format(\n",
    "        acc_test * 100, loss_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_capsule_network\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAACPCAYAAADeIl6VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEcJJREFUeJzt3XuMVMWewPFfMeAg8mYErjCAXmADS5BExvGBG5QLiyhC\nIj6DigiuLwYVhLA+Q1ZRL+qiKCDsjggKMU6AhUWQEBFBjYgiaOSxMDx8oAwgyHOQe/aPbsuq43Rz\npqsfp7u/n2SSX1Gn61RP0Ycfp6rrKM/zBAAAAImpk+kOAAAAZDOSKQAAAAckUwAAAA5IpgAAAByQ\nTAEAADggmQIAAHBAMmVQSq1SSo1I92uRGoxn7mAscwdjmVsYz4icTaaUUjuVUn/LdD/8lFL/rZTy\nlFIdM92XbMJ45o6wjaVSapRSqlIpdVgp9blSqlem+5QtwjSWSqm/KKX+Ryn1Q/Qz2SHTfco2YRpP\nUzZcZ3M2mQqj6EX6r5nuB5KD8cx+SqlSEXlWRIaISBMR+S8RWaCUKshox5CIf4jIMhG5PtMdQfJk\ny3U2r5IppVQzpdQSpdQ+pdTBaNzWd9hflVKfRf+Xukgp1dx4/SVKqY+VUr8opb5SSvWuxbnrisgr\nIjIqOe8GjGfuyOBYdhCRbzzPW+9FHgfxpogUiUjLZLyvfJSpsfQ87yfP814TkXVJfDt5j+tsMHmV\nTEnk/ZaLSHsRaScix0Vkqu+Y20VkuIj8RUR+E5GXRUSUUm1E5H9F5D9EpLmIjBWRCqXUuf6TKKXa\nRf/itDP++CERWe153sakvqP8xnjmjkyN5XsiUqCUKo3ejRouIhtEZG9y315eyeTnEsnHdTYIz/Ny\n8kdEdorI385wTA8ROWiUV4nIs0a5q4hUi0iBiIwXkTm+1y8XkTuM146IcZ5iEfk/EWkSLXsi0jHT\nv6Ns+mE8c+cnZGOpROTfReSURP4RqBKRkkz/jrLlJ0xjaRxfN/qZ7JDp30+2/YRpPLPtOptXd6aU\nUg2UUjOUUruUUodFZLWINPWtj9hjxLtEpJ5Ebvu3F5EbopnzL0qpX0Skl0Qy8TP5TxGZ6HneoeS8\nE4gwnrkkg2N5l0T+R/3PInKWiAwVkSVKqfPc31V+yuBYIgW4zgaTV8mUiIwRkX8SkVLP8xqLyL9E\n/1wZxxQbcTuJ/I+1SiJ/WeZ4ntfU+DnH87xnA5y3j4j8XSm1Vyn1+/TBJ0qpW53eDRjP3JGpsewh\nIos9z9vqed4/PM9bJiI/ishlrm8oj2VqLJEaXGcDyPVkqp5Sqv7vPyLSTCLzvb9EF8g9WcNrhiql\nuiqlGojIRBF51/O80yIyV0QGKqX+VSlVEG2zdw0L8WrSWUQulMiFu0f0zwaKyALH95dvGM/cEZax\nXCci1yilLlARfSUyvl8n5V3mh7CMpUTPXxgtFkbLqJ2wjGdWXWdzPZlaKpG/BL//NBWRsyWSMX8q\nka/R+s0RkTcksgC1voiUiYh4nrdHRAZJZH3FPolk3I9IDb/D6EK6I78vpPM872fP8/b+/hM9rMrz\nvONJep/5gvHMHaEYS4l8e2++RNZuHJbIwtl/8zxvcxLeY74Iy1hK9PxHovHmaBm1E4rxzLbrrIou\n7AIAAEACcv3OFAAAQEqRTAEAADggmQIAAHBAMgUAAOCAZAoAAMBB3TSfj68OZp468yGBMJaZl6yx\nFGE8w4DPZu7gs5lbzjie3JkCAABwQDIFAADggGQKAADAAckUAACAA5IpAAAAByRTAAAADkimAAAA\nHJBMAQAAOEj3pp0AAAAJe/jhh63yG2+8oeMDBw6kuTcR3JkCAABwQDIFAADggGQKAADAAWum0sCc\nzxURGT58uI4HDx5s1b355ps6btiwYUr7BQBANvjqq690PHfuXKvupptuSnd3/oQ7UwAAAA5IpgAA\nABwwzZcGzz33nFUuKCjQ8eLFi626srIyHb/88stWHdN+mWGOw+jRo626q6++WsdLly5NW58Q365d\nu3Q8dOhQq27NmjU6/vLLL626Hj16pLZjqNHp06d1fM0111h127dv1/HatWutupYtW6a2YwiN8vJy\nHR87dsyq8/+dyQTuTAEAADggmQIAAHBAMgUAAOCANVNpsGXLFqtsrpnymzNnjo579+5t1d1+++1J\n7ReCMR9PoJSy6lauXKnjFStWWHV9+/ZNbcfy3KlTp3Q8e/Zsq+6RRx7R8aFDh6w6cwwXLVpk1bFm\nKjMqKyt1bK5pExE5evSojvfv32/VsWYqd3388cdWedq0aTru1KmTVReGay13pgAAAByQTAEAADjI\n+mk+c3fxiRMnWnWe5+l4/vz5Vl1paWlK+2VOMyC7bNq0ySo///zzMY+trq7WsX86Nwy3nnPZK6+8\nouOxY8cm1MaLL75olc12zjnnnMQ6hlrr2LGjjq+66iqrzr99DPLDvHnzrLJ5rTW3pBERKSwsTEuf\n4uHOFAAAgAOSKQAAAAckUwAAAA6ybs3UwoULrfKDDz6oY/MrtCL2IwpOnjyZ2o75+L/Ci+Txr2ky\ny/6nh8fbhiKWSZMmWeUTJ07EPPbyyy/X8Q033FDrcyG+77//3irPmjVLx/7HNCXi119/tcpTp07V\n8fjx453bRzA7d+7U8SeffJK5jiCjzGttvLVyXbp0SUd3aoU7UwAAAA5IpgAAABxk3TTfhg0brPLh\nw4djHmtujWDG6RDv3Ob0Y9A28Ifu3btbZXNH64svvtiqM79yHdT69esDHzto0CAdt2rVqtbnwp8d\nP35cxyUlJVbd3r17A7Vxzz336Hjjxo1WnX9nZWTesWPHdFxVVRXzOHMrHJHkTPUiPCZPnqzjXbt2\nWXXFxcU69i/nCAPuTAEAADggmQIAAHBAMgUAAOAg69ZMmetjROJ/9d1cm+R/XaqZ56tNn2O1ke/i\nPdLFXFv21ltvWXVPPvlkoPZXr16tY/+6HLP9Nm3aWHU8Nsid/zE8jz32mI6DrpHy++abb3T8xRdf\nBH6duS3Gjh07rLqnn35ax0VFRQn1C278nz+4++2336zy22+/reMjR45YdUOGDNFxy5Ytk96XdevW\nxazr1q2bjsP4qCfuTAEAADggmQIAAHCQddN8yA/btm2zyk888UTMY83p0ER3Ia+oqNCxf1dss33/\nE+3hbtWqVVbZHIt4evbsqeN7773XqjN3T463g72fudXKzJkzrbpRo0bpmGm+zOjbt2+mu5BzysrK\nrPK0adN0XKeOfb+ltLRUx8mY5lu0aJFVfv/992Mea04xhhF3pgAAAByQTAEAADggmQIAAHCQ02um\nWrdureMmTZok1Ia5nsP/mINx48bFfN2BAwcSOh8iNm/ebJWrq6tjHnvFFVfouFOnToHaP3r0qFVe\nuHBhzGPr1v3jY+J/XA0Sc+jQIR1PnTo18Os6d+6s42XLlul47ty51nErV65MqF8NGjTQsX8d1gUX\nXJBQm3BTv359HZufRSTu66+/1nF5eXnM4x5//HGrfNFFFyW1H8uXL7fK5vrGpk2bWnVhXy/HnSkA\nAAAHJFMAAAAOsuKeqTm95v8qZTz9+/fX8YUXXhjzOP8u1vv379ex+ZTyoDuXw92MGTMCHztr1iwd\n16tXL9BrVqxYYZX37NkT81hziviBBx4I3C/E9sMPP+h469atgV9nHmtO6R48eDChftx6661WecqU\nKTpu0aJFQm0iufr166fjoNP4iG/JkiU6jrd1SCq2I/j55591PGfOnJjHjRkzxioXFxcnvS/JxJ0p\nAAAAByRTAAAADrJims/c1XrDhg2BX2d+S8GcrqsN8yG35oOTE22jNu34X5frzN2nazP1E3Q6prKy\nUscvvPBC4PbNHdEHDBhg1Zm7cE+cODFwm/muS5cuOr7++uutuvnz5wdqI9GpPdNnn31mlRs2bOjc\nJhB2u3fvjllnPlC4a9euST/39OnTdex/kLL5Db5hw4Yl/dypxJ0pAAAAByRTAAAADkimAAAAHIRy\nzZS5s7GIyMaNG3Vcm+0JzLVJiW5rYLZRWFho1Z133nk6PnnypFX3008/1dhGbfqilArcz1xg/s62\nbdsW+HXm13c/+OCDpPZJxB7b9957z6ozy/7d0a+99tqk9yUX+XdAv/TSS3U8evToQG0UFRVZ5bKy\nMh2ba+VE4u/4jPDZu3evjs31iyIijRo1Snd3csKCBQti1pn/7tSpk/z7LTt37oxZd8stt+i4bdu2\nST93KnFnCgAAwAHJFAAAgINQTvOZt3VF7K/Mp9tTTz2lY/+DF82pBPOByCLhfyhj2MWb4vRvGWH+\n7oNOjfrbSHRK1Wxn8uTJVh3TfME0b97cKt988806jjfN16xZMx37p1/NB7L6n3BgMh+SLfLnqXxk\nnrl9xXfffWfVmVtsILgJEybo2P8Z27Jli45fe+01q+6+++6r9bk+/PBDq1xRURHz2Gx+kDx3pgAA\nAByQTAEAADggmQIAAHAQyjVTrVu3tsqNGzfWcaLrp7p3726Vza3qr7vuupiva9++fULnS5S5Lsv/\ne8h15toZ/2Nbli5dmrZ+dOjQwSrfcccdgV7Xq1evFPQm91VVVVnlQYMGxTz23HPP1bH55HtzjZTf\nrFmzHHqHdFi3bl3MOnMNnX99HRIzcuRIHfvXRZlrph566CGrznwUTLzP6UcffaTj9evXW3XmI2T8\nj2/yX/ezCXemAAAAHJBMAQAAOAjlNF///v2t8pQpU3R85513Bm7H/NrzO++8Y9W1atUqwd6lljkd\n2a9fvwz2JP1atGihY//UzOzZs3X8448/Bm7zxIkTOn799dcDveaZZ56xyuY0A5LPnDoQEfn0009j\nHmuOYUlJifO5GdtwiPfkgvvvv1/HYb1uZ5uzzz5bx++++65Vd+ONN+r422+/teo2bdpUY5yo2267\nzSq3bNnSuc1M4c4UAACAA5IpAAAAByRTAAAADkK5Zspv6NChNcYi9tqaESNGpK1PZ3L69Gkd+x9d\nYtb5+Y/NV/5tIcaPH59QO+bXcoOumerZs2dC50Jw27dv13G8rQs6d+5sleNtgWCaN2+ejg8dOmTV\nmWsR+/TpE6g9IFd169bNKpvXTP/6xaBb1Bw/flzHr776aszjhgwZEqi9bMCdKQAAAAckUwAAAA6y\nYpovnjBN7ZkKCgp07J/WM+v8lFIp61M+Wrt2rY7jTaGWlpbquGPHjintE0QmT56s4927d1t1hYWF\nOl6+fLlV17Zt2xrb27p1q1V+6aWXYp57+PDhOq5Th/9Php250z1PGUg9c9uEK6+80qrzl2O5++67\nY9aZTxW55JJLatm78OJKAgAA4IBkCgAAwAHJFAAAgIOsXzOVawYPHpzpLuSUiooKHZvr0fzrp3Lp\nK7rZrm7dPy5L5voKv5MnT+q4vLzcqvv888+T3zGkTLt27WLW+R9pgvDbt29fzLoBAwbouEGDBuno\nTlpwZwoAAMAByRQAAIADpvmSZNy4cTHrevfubZUnTJigY/8Oz23atElqv/JddXV1oOOKiopS3BME\nZY7ZpEmTrDpzCnDx4sU6XrNmTcz2hg0bZpWZ0g2fkSNH6njmzJlW3erVq3VcWVlp1Z1//vmp7RiS\nLlc/f9yZAgAAcEAyBQAA4IBkCgAAwAFrppKkqqrKKt911106nj59erq7g6izzjor0HEDBw5McU9g\nKi4ujll36tQpHT/66KMJtW+uk5oxY4ZVxyNkwsf8+1BSUmLVmWvjTpw4kbY+IXELFizIdBfSjqsK\nAACAA5IpAAAAB0zzJcmOHTsy3QXUwPyadZ8+fXR82WWXWcc1atQobX2CyJgxY3TcuHFjq66srCxQ\nG4WFhTr2TweOHTtWx/Xq1Uuki8gQ//ib03xAWHFnCgAAwAHJFAAAgAOSKQAAAAfK87x0ni+tJ0ON\nVJLaYSwzL1ljKcJ4hgGfzdzBZzO3nHE8uTMFAADggGQKAADAAckUAACAA5IpAAAAByRTAAAADkim\nAAAAHJBMAQAAOCCZAgAAcEAyBQAA4CDdO6ADAADkFO5MAQAAOCCZAgAAcEAyBQAA4IBkCgAAwAHJ\nFAAAgAOSKQAAAAckUwAAAA5IpgAAAByQTAEAADggmQIAAHBAMgUAAOCAZAoAAMAByRQAAIADkikA\nAAAHJFMAAAAOSKYAAAAckEwBAAA4IJkCAABwQDIFAADggGQKAADAAckUAACAA5IpAAAAByRTAAAA\nDv4fjjClYGWu0vYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1408e6fd68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAACPCAYAAADeIl6VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUwAAAAA9YDEFAAAA0IP/ByITTa7SBGasAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1408945780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predictions\n",
    "\n",
    "n_samples = 5\n",
    "\n",
    "sample_images = mnist.test.images[:n_samples].reshape([-1, 28, 28, 1])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "    caps2_output_value, decoder_output_value, y_pred_value = sess.run(\n",
    "            [caps2_output, decoder_output, y_pred],\n",
    "            feed_dict={X: sample_images,\n",
    "                       y: np.array([], dtype=np.int64)})\n",
    "    \n",
    "sample_images = sample_images.reshape(-1, 28, 28)\n",
    "reconstructions = decoder_output_value.reshape([-1, 28, 28])\n",
    "\n",
    "plt.figure(figsize=(n_samples * 2, 3))\n",
    "for index in range(n_samples):\n",
    "    plt.subplot(1, n_samples, index + 1)\n",
    "    plt.imshow(sample_images[index], cmap=\"binary\")\n",
    "    plt.title(\"Label:\" + str(mnist.test.labels[index]))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(n_samples * 2, 3))\n",
    "for index in range(n_samples):\n",
    "    plt.subplot(1, n_samples, index + 1)\n",
    "    plt.title(\"Predicted:\" + str(y_pred_value[index]))\n",
    "    plt.imshow(reconstructions[index], cmap=\"binary\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
